version: "3"

services:

  metrics-collector:
    image: "metrics-collector:latest"
    hostname: metrics-collector
    environment:
      - MODE=streaming
      - FAULT=no_fault
      - TEST_NUMBER=1
    deploy:
      placement:
        constraints:
          - node.labels.role==manager
    networks:
      - spark-network
    depends_on:
      - mariadb

  mariadb:
    image: "maria-initialized:latest"
    hostname: spark-mariadb
    command: --character-set-server=utf8 --lower_case_table_names
    ports:
      - "3306:3306"
    environment:
      - MYSQL_ROOT_PASSWORD=spark
      - MYSQL_DATABASE=spark
      - MYSQL_USER=spark
      - MYSQL_PASSWORD=spark
    deploy:
      placement:
        constraints:
          - node.labels.role==manager
    networks:
      - spark-network

  spark-master:
    image: "spark-with-metrics:latest"
    hostname: spark_master
    ports:
      - "8080:8080"
    environment:
      - SPARK_MODE=master # default
    deploy:
      placement:
        constraints:
          - node.labels.role==master
    networks:
      - spark-network

  spark-worker-1:
    image: "spark-with-metrics:latest"
    hostname: spark_worker_1
    ports:
      - "8081:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=6
      - SPARK_WORKER_MEMORY=12G
      - SPARK_MASTER_URL=spark://spark-master:7077
    deploy:
      placement:
        constraints:
          - node.labels.role==worker-1
      replicas: 1
    networks:
      - spark-network

  spark-worker-2:
    image: "spark-with-metrics:latest"
    hostname: spark_worker_2
    ports:
      - "8082:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=6
      - SPARK_WORKER_MEMORY=12G
      - SPARK_MASTER_URL=spark://spark-master:7077
    deploy:
      placement:
        constraints:
          - node.labels.role==worker-2
      replicas: 1
    networks:
      - spark-network

  spark-worker-3:
    image: "spark-with-metrics:latest"
    hostname: spark_worker_3
    ports:
      - "8083:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=6
      - SPARK_WORKER_MEMORY=10G
      - SPARK_MASTER_URL=spark://spark-master:7077
    deploy:
      placement:
        constraints:
          - node.labels.role==worker-3
      replicas: 1
    networks:
      - spark-network

  driver:
    image: "driver:latest"
    ports:
      - "4040:4040"
    user: root # This fixes KerberosAuthException during spark-submit: https://issueexplorer.com/issue/bitnami/bitnami-docker-spark/27
    deploy:
      placement:
        constraints:
          - node.labels.role==master
    networks:
      - spark-network
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
      - kafka_1
    environment:
      - TZ=Europe/Berlin
      - AWS_ACCESS_KEY_ID=access_key_id
      - AWS_SECRET_ACCESS_KEY=secret_access_key

  generator:
    image: "generator:latest"
    hostname: generator
    ports:
      - "8091:8080"
    environment:
      - BOOTSTRAP_KAFKA_SERVER=swarm-cluster_kafka_1:9092
      - THROUGHPUT=25000
    deploy:
      restart_policy:
        condition: none
      placement:
        constraints:
          - node.labels.role==manager
    networks:
      - spark-network
    depends_on:
      - driver

  #gremlin-driver:
  #  image: "gremlin/gremlin:latest"
  #  hostname: gremlin-driver
  #  environment:
  #  - GREMLIN_TEAM_ID=8882f35b-8baa-4265-82f3-5b8baa82659b
  #  - GREMLIN_TEAM_SECRET=6e06d523-16ec-4c00-86d5-2316ecec00f0
  #  tty: true
  #  deploy:
  #    placement:
  #      constraints:
  #        - node.labels.role==manager
  #  networks:
  #    - spark-network

  kafka_1:
    image: "wurstmeister/kafka:latest"
    hostname: kafka_1
    ports:
      - "29092:29092"
      - "7071:7071"
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_LISTENERS=HOST_INTERNAL://:29092, DOCKER_INTERNAL://kafka_1:9092 # Makes Kafka broker accessible in the docker overlay network (no external): https://rmoff.net/2018/08/02/kafka-listeners-explained/
      - KAFKA_ADVERTISED_LISTENERS=HOST_INTERNAL://172.24.40.133:29092, DOCKER_INTERNAL://kafka_1:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=HOST_INTERNAL:PLAINTEXT, DOCKER_INTERNAL:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=DOCKER_INTERNAL
    deploy:
      placement:
        constraints:
          - node.labels.role==manager
    networks:
      - spark-network
    depends_on:
      - zookeeper

  kafka_2:
    image: "wurstmeister/kafka:latest"
    hostname: kafka_2
    ports:
      - "29093:29093"
      - "7073:7071"
    environment:
      - KAFKA_BROKER_ID=2
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_LISTENERS=HOST_INTERNAL://:29093, DOCKER_INTERNAL://kafka_2:9092 # Makes Kafka broker accessible in the docker overlay network (no external): https://rmoff.net/2018/08/02/kafka-listeners-explained/
      - KAFKA_ADVERTISED_LISTENERS=HOST_INTERNAL://172.24.40.133:29093, DOCKER_INTERNAL://kafka_2:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=HOST_INTERNAL:PLAINTEXT, DOCKER_INTERNAL:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=DOCKER_INTERNAL
    deploy:
      placement:
        constraints:
          - node.labels.role==manager
    networks:
      - spark-network
    depends_on:
      - zookeeper

  kafka_3:
    image: "wurstmeister/kafka:latest"
    hostname: kafka_3
    ports:
      - "29094:29094"
      - "7074:7071"
    environment:
      - KAFKA_BROKER_ID=3
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_LISTENERS=HOST_INTERNAL://:29094, DOCKER_INTERNAL://kafka_3:9092 # Makes Kafka broker accessible in the docker overlay network (no external): https://rmoff.net/2018/08/02/kafka-listeners-explained/
      - KAFKA_ADVERTISED_LISTENERS=HOST_INTERNAL://172.24.40.133:29094, DOCKER_INTERNAL://kafka_3:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=HOST_INTERNAL:PLAINTEXT, DOCKER_INTERNAL:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=DOCKER_INTERNAL
    deploy:
      placement:
        constraints:
          - node.labels.role==manager
    networks:
      - spark-network
    depends_on:
      - zookeeper

  grafana:
    image: "grafana-dashboards:latest"
    hostname: spark_grafana
    ports:
      - "3000:3000"
    deploy:
      placement:
        constraints:
          - node.labels.role==manager
    networks:
      - spark-network

  #prometheus:
  #  image: "prometheus-configs:latest"
  #  hostname: prometheus
  #  ports:
  #    - "9090:9090"
  #  depends_on:
  #    - kafka_1
  #  deploy:
  #    placement:
  #      constraints:
  #        - node.labels.role==manager
  #  networks:
  #    - spark-network

  zookeeper:
    image: "zookeeper:latest" # This image is configured with volumes at /data and /datalog to hold the Zookeeper in-memory database snapshots and the transaction log of updates to the database
    hostname: zookeeper
    ports:
      - "2181:2181"
    environment:
      - ZOO_LOG4J_PROP="INFO,ROLLINGFILE" # Redirects logging to /logs/zookeeper.log, the image already has a volume in /logs
    deploy:
      placement:
        constraints:
          - node.labels.role==manager
    networks:
      - spark-network

  viz:
    build: https://github.com/dockersamples/docker-swarm-visualizer.git
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    ports:
      - "8080:8080"
    deploy:
      placement:
        constraints:
          - node.labels.role==manager

  #node-exporter-1:
  #  image: bitnami/node-exporter
  #  deploy:
  #    placement:
  #      constraints:
  #        - node.labels.role==master
  #  ports:
  #    - "9100:9100"
#
  #node-exporter-2:
  #  image: bitnami/node-exporter
  #  deploy:
  #    placement:
  #      constraints:
  #        - node.labels.role==worker-1
  #  ports:
  #    - "9101:9100"
#
  #node-exporter-3:
  #  image: bitnami/node-exporter
  #  deploy:
  #    placement:
  #      constraints:
  #        - node.labels.role==worker-2
  #  ports:
  #    - "9102:9100"
#
  #node-exporter-4:
  #  image: bitnami/node-exporter
  #  deploy:
  #    placement:
  #      constraints:
  #        - node.labels.role==worker-3
  #  ports:
  #    - "9103:9100"

networks:
  spark-network:
    driver: overlay
