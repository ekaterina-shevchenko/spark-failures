version: "3"

services:

  spark-master:
    image: "bitnami/spark:3.2.0"
    container_name: spark_master
    ports:
      - "8080:8080"
    environment:
      - SPARK_MODE=master # default
    deploy:
      placement:
        constraints:
          - node.labels.role==master
    networks:
      - spark-network

  spark-worker:
    image: "bitnami/spark:3.2.0"
    container_name: spark_worker
    ports:
      - "8081:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=3G
    deploy:
      placement:
        constraints:
          - node.labels.role==worker
      replicas: 1
    networks:
      - spark-network

  driver:
    image: "driver:latest"
    container_name: spark_driver
    ports:
      - "8090:8080"
    user: root # This fixes KerberosAuthException during spark-submit: https://issueexplorer.com/issue/bitnami/bitnami-docker-spark/27
    deploy:
      placement:
        constraints:
          - node.labels.role==manager
    networks:
      - spark-network
    volumes:
      - ./logs/checkpoint:/opt/bitnami/checkpoint # structured streaming will save checkpoints in this location
    depends_on:
      - kafka

  generator:
    image: "generator:latest"
    container_name: generator
    ports:
      - "8091:8080"
    environment:
      - BOOTSTRAP_KAFKA_SERVER=kafka:9092
    deploy:
      placement:
        constraints:
          - node.labels.role==manager
    networks:
      - spark-network
    depends_on:
      - kafka

  kafka:
    image: "wurstmeister/kafka:latest"
    container_name: kafka
    ports:
      - "29092:29092"
    environment:
      - KAFKA_CREATE_TOPICS="output:3:1" # topic_name:number_of_partitions_number_of_replicas
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_LISTENERS=HOST_INTERNAL://:29092, DOCKER_INTERNAL://kafka:9092 # Makes Kafka broker accessible in the docker overlay network (no external): https://rmoff.net/2018/08/02/kafka-listeners-explained/
      - KAFKA_ADVERTISED_LISTENERS=HOST_INTERNAL://localhost:29092, DOCKER_INTERNAL://kafka:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=HOST_INTERNAL:PLAINTEXT, DOCKER_INTERNAL:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=DOCKER_INTERNAL
    deploy:
      placement:
        constraints:
          - node.labels.role==manager
    networks:
      - spark-network
    volumes:
      - ./logs/kafka:/kafka # Keep this volume if you don't want to run into device-out-of-memory error. Otherwise you would need either to delete Kafka topic's data via Producer API call, or navigate to docker's /var/lib/docker/volumes/ and do it manually (instructions: https://stackoverflow.com/questions/38532483/where-is-var-lib-docker-on-mac-os-x)
    depends_on:
      - zookeeper

  zookeeper:
    image: "zookeeper:latest" # This image is configured with volumes at /data and /datalog to hold the Zookeeper in-memory database snapshots and the transaction log of updates to the database
    container_name: spark_failures_zookeeper
    ports:
      - "2181:2181"
    restart: always # Since the Zookeeper "fails fast" it's better to always restart it
    environment:
      - ZOO_LOG4J_PROP="INFO,ROLLINGFILE" # Redirects logging to /logs/zookeeper.log, the image already has a volume in /logs
    deploy:
      placement:
        constraints:
          - node.labels.role==manager
    networks:
      - spark-network

networks:
  spark-network:
    driver: overlay